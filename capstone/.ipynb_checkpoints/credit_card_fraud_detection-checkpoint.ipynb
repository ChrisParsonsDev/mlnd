{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Nanodegree\n",
    "\n",
    "## Project Capstone\n",
    "\n",
    "## Credit Card Fraud Detection\n",
    "\n",
    "\n",
    "Welcome to the final project of my Machine Learning Engineer Nanodegree! In this notebook, I'll explore whether or not a system can be trained to detect instances of financial fraud from a labeled dataset of credit card transactions. As such this is project is an example of Supervised Learning. To thorougly examine credit card fraud detection systems I will explore the effecitveness of the following techniques: \n",
    "\n",
    "\n",
    "1. **Benchmark Model** - The benchmark model will be a [Logistic Regression Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). This model will be implemented using the tooling in SKLearn and be used to compare all other models developed as part of this project.\n",
    "2. **Complex Model** - I will then explore how using models like SVM or [xgboost](https://github.com/dmlc/xgboost) perform when applied to this binary classification problem.\n",
    "3. **Neural Network** - In order to examine the effectiveness of complex solutions on this non-image domain I will develop a Neural Network architecture that is capable of classifying the instances of fraud. Subsequent analysis of the accuracy of the Neural Network architecture will allow direct comparison between complex and simple techniques in this problem space.\n",
    "4. **AutoEncoders with TensorFlow** - Finally, in order to mitigate issues with the Credit Card Fraud dataset that I observed during this project I'll explore how AutoEncoders and Unsupervised learning can be applied to this domain. \n",
    "\n",
    "\n",
    "More information can be found in the project's [proposal.md](https://github.com/ChrisParsonsDev/mlnd/blob/master/capstone/PROPOSAL.md) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing The Data\n",
    "\n",
    "In order to begin working with the [Credit Card Fraud](https://www.kaggle.com/dalpozz/creditcardfraud) data, I'll first need to import the required functionality and load the .csv data into a pandas DataFrame. The code in the following cell will load my data and display the first few entries for examination using panda's .head() function. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...         V21       V22       V23       V24  \\\n",
       "0  0.098698  0.363787  ...   -0.018307  0.277838 -0.110474  0.066928   \n",
       "1  0.085102 -0.255425  ...   -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.247676 -1.514654  ...    0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.377436 -1.387024  ...   -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4 -0.270533  0.817739  ...   -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  Class  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Import supplementary visualization code visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the dataset\n",
    "in_file = 'datasets/creditcard.csv'\n",
    "raw_data = pd.read_csv(in_file)\n",
    "\n",
    "# Print the first few entries of the Credit Card fraud data\n",
    "display(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this subset of the Credit Card transation data, we can see various features for each transaction in the dataset: \n",
    "\n",
    "- **Class: ** The class label for the datapoint. Positive class (1) indicates an incident of credit card fraud and the Negative Class (0) is not an instance of fraud. \n",
    "- ** Time: ** The time in seconds between each transaction.\n",
    "- ** V1 - V28: ** Due to the sensitive nature of financial information it is not possible to know specifics about each of the features in this dataset. However we do know that the data itself has been cleaned of extraneous values and has undergone Principle Component Analysis (PCA). \n",
    "- ** Amount: ** The amount of the transaction. \n",
    "\n",
    "The code cell below removes the Class field and stores it as \"labels\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9   ...         V20       V21       V22       V23  \\\n",
       "0  0.098698  0.363787   ...    0.251412 -0.018307  0.277838 -0.110474   \n",
       "1  0.085102 -0.255425   ...   -0.069083 -0.225775 -0.638672  0.101288   \n",
       "2  0.247676 -1.514654   ...    0.524980  0.247998  0.771679  0.909412   \n",
       "3  0.377436 -1.387024   ...   -0.208038 -0.108300  0.005274 -0.190321   \n",
       "4 -0.270533  0.817739   ...    0.408542 -0.009431  0.798278 -0.137458   \n",
       "\n",
       "        V24       V25       V26       V27       V28  Amount  \n",
       "0  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Store the 'Class' feature in a new variable and remove it from the dataset\n",
    "labels = raw_data['Class']\n",
    "features = raw_data.drop('Class', axis = 1)\n",
    "\n",
    "# Show the new dataset with 'Class' removed\n",
    "display(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The sample produced by the '.head()' function now shows the Class feature removed from the DataFrame. For any single transaction 'features.loc[i]' the label is labels[i]. In order to appropriately compare the different Machine Learning techniques and models I must have some way of measuring their performance. Since the key success factor for this workload is how accurately it can predict or detect instances of fraud. I will explore three different performance metrics - Precision, Recall and Fscore. \n",
    "\n",
    "* Precision = TP/(TP+FP)\n",
    "* Recall = TP/(TP+FN)\n",
    "* [FScore](https://en.wikipedia.org/wiki/F1_score)\n",
    "\n",
    "The function below can be used to compute these metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def accuracy(truth, pred):\n",
    "    \"\"\" Returns accuracy score for input truth and predictions. \"\"\"   \n",
    "    # Ensure that the number of predictions matches number of outcomes\n",
    "    if len(truth) == len(pred): \n",
    "        precision, recall, fscore, support = precision_recall_fscore_support(truth, pred, average='binary')\n",
    "        return \"Precision: {:.2f}, Recall: {:.2f}, Fscore: {:.2f}\".format(precision, recall, fscore)\n",
    "    else:\n",
    "        return \"Number of predictions does not match number of outcomes!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining The Dataset\n",
    "\n",
    "In order to ensure the validity of any models produced, it is important to have a thorough understanding of the dataset itself. With Supervised Learning, specifically binary classification problems such as this, the distribution of samples in both positive and negative classes is pivotal. The code cell below explores this distribution.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 284807 samples, 492 of which are in the positive class\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1139fc5d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAETCAYAAADge6tNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAHbhJREFUeJzt3X+8VXWd7/HXW1D8jSBEegTRwCa0NEW0qSaLBKqZ0Dvq\nYKVcx6SuNjenacYfY2I6THpvqeOUliWhWCppKjNqhpqaMykcHUYE9XJUjF8CCoGYoMDn/rG+Wxfb\nc/bZ/PieLZv38/HYj7P2d631Xd+19jn7vdd3fc/aigjMzMxy2qHRDTAzs+bnsDEzs+wcNmZmlp3D\nxszMsnPYmJlZdg4bMzPLzmFjnZI0T9Kn0/T5kn7S6DaVSfqfkh7ZgvUflPTlrdmmLSVptaQDt1Jd\nb71mkgZKCkndt1LdA1Jbu22N+jZj+1vtOFleDpsmIOkLklrTH95iSfdI+liObUXEP0fEl9N263rj\nknSQpF9IelnSSklPSvpGV71BSdpJ0kWS5kp6LYXnREkDu2L7VW05RtKG9FqtlrRA0hRJR5aXi4jd\nI+L5Oupa0Nk2y6/Zlip/8Eh1/z61df3WqL9qWyFpUFXZRZJuLG1/qx0ny8ths42T9A3gSuCfgX7A\nAOAHwOc7WH6rfKKtl6T3AY8B84EPRkRP4ETgCGCPzahvcwLqVorj8QWgJ3Ao0AoM34y6toZFEbE7\nxf4fDTwD/FbSVm9PV7/e26NGndVtcyLCj230QfHGuRo4scYyF1G82d4IrAK+TPEh41zgOeAVYArQ\nu7TOKcCLad4/AvOAT5fquzFN/x6I1IbVwEfa2f6NwF2d7McvgJeAlcDDwMGleZOAa4C7gdeATwN7\nA1PT/kwHLgEe6aDuTwOvA/1rbP9B4Mtp+n3AA2nfXwZ+BuxVWvYcYCHwKvAsMDyVD6MIsFXAEuDy\nDrZ1DLCgnfLvA62l5wEMStOfBeakbS4EvgnslvZrQ+n479vB611+zQamuscBi4DFwDerjvc/tdde\nYHLa3utpe/9Qqq97Wmbf9NosB9qAM6p+F6cAN6R9mQ0MrfG6vHUMquq4cSscpx4UH9IWpceVQI9S\nvf+Qjs2idAzL25nEO38nPwf8Vzrm84GLSnVVjtFpad4K4KvAkcCTwB+A7zf6/ST3o+EN8GMLXjwY\nBayr/KF3sMxFwJvAcRQhswvwdeBRYL/0R/cj4Ka0/JD0B/lnad7laRvthc1GbzQdbP8l4LRO9uOv\nKT7lV94AZpbmTaIIoY+m9u8M3JzetHYDDklvLB2FzaXAQ51s/0HeDptBwLGpLX0pwu/KNO/96c1i\n39L+vy9N/w44JU3vDhzdwbaOof2w+RTFG+Ju6Xn5zW0x8PE03Qs4vKO6Oni923vNbkrH74PAstLr\nO4kOwiY9n1dZtr3fgXS8rk6v02Gp7k+V2raGIhS6Ad8BHq3xumxq2GzKcbqY4m/gPel1/k/gktLf\n1UvAwcCuFMFdHTbVv5PHpGO5A/Ahig8cx1Udox+mZUek43BH2n4LsBT4RKPfU3I+3I22bdsbeDki\n1nWy3O8i4o6I2BARr1N8qvrHiFgQEWsp/oBPSF0uJwD/HhEPp3nfongT3JI2Lq61QERMjIhXS205\nVFLP0iJ3RsR/RMQGijfSvwQujIjXIuIp4Pot2X5VW9oiYlpErI2IZRRh+4k0ez1FCA2RtGNEzIuI\n59K8N4FBkvpExOqIeLTebSaLAAF7tTPvzbTNPSNiRUQ80Uld1a93e76djt8s4KfAyZvY3neQ1J/i\nDficiFgTETOBnwCnlhZ7JCLujuIaz2SKLs1anpD0h8qD4oy8I5tynL4IXBwRS9Pr/G2KM3qAk4Cf\nRsTsiPgjxe9ktbd+J9O+PhgRs9LzJynC/BNV61ySlv01xRnRTWn7C4HfAh/u5Fhs0xw227ZXgD51\n9MvPr3q+P3B76Q/4aYo30n4UXQxvLR8Rr6XtbEkb9+lopqRuki6V9JykVRSfnAH6dND+vkD3qrIX\nN3f77bSnn6SbJS1M7bmx0paIaAPOpnjzWZqW2zetejpwEPCMpBmS/rzebSYtFJ9+/9DOvL+kOBt4\nUdJDkj7SSV3Vr3dny7xI8bpvqX2B5RHxalXdLaXnL5Wm/wjs3Mnv7+ERsVflQXGm2pFNOU77svHv\nTfkYbPQ3QPvHc6MySUdJ+o2kZZJWUnyg61O1zpLS9OvtPN+9Rnu3eQ6bbdvvgLUUXSa1VN/aez7w\nmfIfcUTsnD5hLQb6VxaUtCvF2UE99bbnPoo3gY58ARhN0e/dk6LLAYpP+e1tZxlFt17/UtmATrY/\nTNJ+dbQVioEWQTGYYU/gS+W2RMTPI+JjFIEdwGWpfG5EnEzRLXIZcKuk3ercJsDxwBMp3DcSETMi\nYnSq+w6KLkTo+PjX87pUH79Fafo1iq6jivduQt2LgN6SygM/BlB0c2a3icdpEcVrWFE+Bospupgr\nysfqrc1VPf85xbWq/lEMgvkhG/8Ob/ccNtuwiFgJXAj8QNJxknaVtKOkz0j6PzVW/SEwQdL+AJL6\nShqd5t0K/Lmkj0naiaJvu6Pfk2UUXWy1/s9hPPCnkv6vpPem7Q2SdKOkvSiu1aylOAPZleLNvtY+\nrwd+CVyU9ncIMLbG8vcB0yjO5I6Q1F3SHpK+Kumv21llD4prVisltQB/X5kh6f2SPiWpB0Wfe+XC\nM5K+JKlv6uqrnJ3U7H5UoUXSeIqL0Oe3s8xOkr4oqWdEvElxAbpS7xJg76oux3p9Kx2/gykuXN+S\nymcCn5XUO71eZ1ett4QOXu+ImE9x7eM7knaW9CGKM74b21t+a9qM43QTcEH63e9D8XdUaecU4DRJ\nH0gftr5VRxP2oDirWyNpGMWHKCtx2GzjIuJ7wDeACyje/OcDX6P4ZNeRf6H4FPZrSa9SXCg9KtU3\nGziL4pPaYoqRM+3+j0Lqz54A/Efqkju6nWWeAz5CccYyO3Ux3EYxcutVipFJL1J8+p2T2tKZr1F0\nObxEcbH2p50sfwLFyKFbKC7sPgUMpTjrqfZt4PC03F0UwVbRg6Ib5+W07fcA56V5o9L+raY4vmNq\nXC/ZNy23GphBcWH5mNSX355TgHmpW++rFNcbiIhnKN40n0/Hf1O6wh6iGC12P/Dd0rYnA/9N0Z35\na94OoYrvULxJ/0HSN9up92SK13oRcDswPgV+V9iU4/RPFL+DTwKzgCdSGRFxD3AV8BuKY1T5nVxb\nY9tnAhenv6cLefusyhJF+MvTzMw6IukDFB9QetQxGMc64DMbM7Mqko6X1ENSL4prcP/moNkyDhsz\ns3f6CsX/vjxHMVLzfzW2Ods+d6OZmVl2PrMxM7PsHDZmZpad7wib9OnTJwYOHNjoZpiZbVMef/zx\nlyOib2fLOWySgQMH0tra2uhmmJltUyTVul3UW9yNZmZm2TlszMwsO4eNmZll57AxM7PsHDZmZpad\nw8bMzLJz2JiZWXYOGzMzy87/1LmNGXjuXY1uQlOZd+nnGt0Es+2Cz2zMzCw7h42ZmWXnsDEzs+wc\nNmZmlp3DxszMsnPYmJlZdg4bMzPLzmFjZmbZOWzMzCw7h42ZmWXnsDEzs+wcNmZmlp3DxszMsnPY\nmJlZdg4bMzPLzmFjZmbZOWzMzCw7h42ZmWXnsDEzs+wcNmZmlp3DxszMsssWNpL6S/qNpDmSZkv6\neiq/SNJCSTPT47Oldc6T1CbpWUkjS+VHSJqV5l0lSam8h6RbUvljkgaW1hkraW56jM21n2Zm1rnu\nGeteB/xdRDwhaQ/gcUnT0rwrIuK75YUlDQHGAAcD+wL3STooItYD1wBnAI8BdwOjgHuA04EVETFI\n0hjgMuCvJPUGxgNDgUjbnhoRKzLur5mZdSDbmU1ELI6IJ9L0q8DTQEuNVUYDN0fE2oh4AWgDhkna\nB9gzIh6NiABuAI4rrXN9mr4VGJ7OekYC0yJieQqYaRQBZWZmDdAl12xS99aHKc5MAP5G0pOSJkrq\nlcpagPml1RakspY0XV2+0ToRsQ5YCexdo67qdo2T1CqpddmyZZu9f2ZmVlv2sJG0O3AbcHZErKLo\nEjsQOAxYDHwvdxs6EhHXRsTQiBjat2/fRjXDzKzpZQ0bSTtSBM3PIuKXABGxJCLWR8QG4MfAsLT4\nQqB/afX9UtnCNF1dvtE6kroDPYFXatRlZmYNkHM0moDrgKcj4vJS+T6lxY4HnkrTU4ExaYTZAcBg\nYHpELAZWSTo61XkqcGdpncpIsxOAB9J1nXuBEZJ6pW66EanMzMwaIOdotI8CpwCzJM1MZecDJ0s6\njGKU2DzgKwARMVvSFGAOxUi2s9JINIAzgUnALhSj0O5J5dcBkyW1AcspRrMREcslXQLMSMtdHBHL\nM+2nmZl1IlvYRMQjgNqZdXeNdSYAE9opbwUOaad8DXBiB3VNBCbW214zM8vHdxAwM7PsHDZmZpad\nw8bMzLJz2JiZWXYOGzMzy85hY2Zm2TlszMwsO4eNmZll57AxM7PsHDZmZpadw8bMzLJz2JiZWXYO\nGzMzy85hY2Zm2TlszMwsO4eNmZll57AxM7PsHDZmZpadw8bMzLJz2JiZWXYOGzMzy85hY2Zm2Tls\nzMwsO4eNmZll57AxM7PsHDZmZpadw8bMzLLLFjaS+kv6jaQ5kmZL+noq7y1pmqS56Wev0jrnSWqT\n9KykkaXyIyTNSvOukqRU3kPSLan8MUkDS+uMTduYK2lsrv00M7PO5TyzWQf8XUQMAY4GzpI0BDgX\nuD8iBgP3p+ekeWOAg4FRwNWSuqW6rgHOAAanx6hUfjqwIiIGAVcAl6W6egPjgaOAYcD4cqiZmVnX\nyhY2EbE4Ip5I068CTwMtwGjg+rTY9cBxaXo0cHNErI2IF4A2YJikfYA9I+LRiAjghqp1KnXdCgxP\nZz0jgWkRsTwiVgDTeDugzMysi3XJNZvUvfVh4DGgX0QsTrNeAvql6RZgfmm1BamsJU1Xl2+0TkSs\nA1YCe9eoy8zMGiB72EjaHbgNODsiVpXnpTOVyN2GjkgaJ6lVUuuyZcsa1Qwzs6aXNWwk7UgRND+L\niF+m4iWpa4z0c2kqXwj0L62+XypbmKaryzdaR1J3oCfwSo26NhIR10bE0IgY2rdv383dTTMz60TO\n0WgCrgOejojLS7OmApXRYWOBO0vlY9IIswMoBgJMT11uqyQdneo8tWqdSl0nAA+ks6V7gRGSeqWB\nASNSmZmZNUD3jHV/FDgFmCVpZio7H7gUmCLpdOBF4CSAiJgtaQowh2Ik21kRsT6tdyYwCdgFuCc9\noAizyZLagOUUo9mIiOWSLgFmpOUujojluXbUzMxqyxY2EfEIoA5mD+9gnQnAhHbKW4FD2ilfA5zY\nQV0TgYn1ttfMzPLxHQTMzCw7h42ZmWXnsDEzs+wcNmZmlp3DxszMsnPYmJlZdg4bMzPLrq6wkfTB\n3A0xM7PmVe+ZzdWSpks6U1LPrC0yM7OmU1fYRMTHgS9S3NzycUk/l3Rs1paZmVnTqPuaTUTMBS4A\nzgE+AVwl6RlJ/yNX48zMrDnUe83mQ5KuoPi2zU8BfxERH0jTV2Rsn5mZNYF6b8T5r8BPgPMj4vVK\nYUQsknRBlpaZmVnTqDdsPge8Xrnlv6QdgJ0j4o8RMTlb68zMrCnUe83mPorvkqnYNZWZmZl1qt6w\n2TkiVleepOld8zTJzMyaTb1h85qkwytPJB0BvF5jeTMzs7fUe83mbOAXkhZRfPvme4G/ytYqMzNr\nKnWFTUTMkPQnwPtT0bMR8Wa+ZpmZWTOp98wG4EhgYFrncElExA1ZWmVmZk2lrrCRNBl4HzATWJ+K\nA3DYmJlZp+o9sxkKDImIyNkYMzNrTvWORnuKYlCAmZnZJqv3zKYPMEfSdGBtpTAiPp+lVWZm1lTq\nDZuLcjbCzMyaW71Dnx+StD8wOCLuk7Qr0C1v08zMrFnU+xUDZwC3Aj9KRS3AHbkaZWZmzaXeAQJn\nAR8FVsFbX6T2nlorSJooaamkp0plF0laKGlmeny2NO88SW2SnpU0slR+hKRZad5VkpTKe0i6JZU/\nJmlgaZ2xkuamx9g699HMzDKpN2zWRsQblSeSulP8n00tk4BR7ZRfERGHpcfdqb4hwBjg4LTO1ZIq\n3XTXAGcAg9OjUufpwIqIGETxBW6Xpbp6A+OBo4BhwHhJvercTzMzy6DesHlI0vnALpKOBX4B/Fut\nFSLiYWB5nfWPBm6OiLUR8QLQBgyTtA+wZ0Q8mv7H5wbguNI616fpW4Hh6axnJDAtIpZHxApgGu2H\nnpmZdZF6w+ZcYBkwC/gKcDewud/Q+TeSnkzdbJUzjhZgfmmZBamsJU1Xl2+0TkSsA1YCe9eoy8zM\nGqSusImIDRHx44g4MSJOSNObczeBa4ADgcOAxcD3NqOOrUbSOEmtklqXLVvWyKaYmTW1ekejvSDp\n+erHpm4sIpZExPqI2AD8mOKaCsBCoH9p0f1S2cI0XV2+0TrpGlJP4JUadbXXnmsjYmhEDO3bt++m\n7o6ZmdWp3m60oRR3fT4S+DhwFXDjpm4sXYOpOJ7iNjgAU4ExaYTZARQDAaZHxGJglaSj0/WYU4E7\nS+tURpqdADyQzrbuBUZI6pW66UakMjMza5B6/6nzlaqiKyU9DlzY0TqSbgKOAfpIWkAxQuwYSYdR\njGSbR3H9h4iYLWkKMAdYB5wVEZW7S59JMbJtF+Ce9AC4DpgsqY1iIMKYVNdySZcAM9JyF0dEvQMV\nzMwsg3q/YuDw0tMdKM50aq4bESe3U3xdjeUnABPaKW8FDmmnfA1wYgd1TQQm1mqfmZl1nXrvjVa+\nkL+O4qzkpK3eGjMza0r1dqN9MndDzMysedXbjfaNWvMj4vKt0xwzM2tGm/JNnUdSjAAD+AtgOjA3\nR6PMzKy51Bs2+wGHR8SrUNxQE7grIr6Uq2FmZtY86v0/m37AG6Xnb6QyMzOzTtV7ZnMDMF3S7en5\ncbx9E0wzM7Oa6h2NNkHSPRR3DwA4LSL+K1+zzMysmdTbjQawK7AqIv4FWJBuK2NmZtapem/EOR44\nBzgvFe3IZtwbzczMtk/1ntkcD3weeA0gIhYBe+RqlJmZNZd6w+aNdEflAJC0W74mmZlZs6k3bKZI\n+hGwl6QzgPsovo/GzMysU/WORvuupGOBVcD7gQsjYlrWlpmZWdPoNGwkdQPuSzfjdMCYmdkm67Qb\nLX2J2QZJPbugPWZm1oTqvYPAamCWpGmkEWkAEfG/s7TKzMyaSr1h88v0MDMz22Q1w0bSgIj4fUT4\nPmhmZrbZOrtmc0dlQtJtmdtiZmZNqrOwUWn6wJwNMTOz5tVZ2EQH02ZmZnXrbIDAoZJWUZzh7JKm\nSc8jIvbM2jozM2sKNcMmIrp1VUPMzKx5bcr32ZiZmW0Wh42ZmWXnsDEzs+wcNmZmll22sJE0UdJS\nSU+VynpLmiZpbvrZqzTvPEltkp6VNLJUfoSkWWneVZKUyntIuiWVPyZpYGmdsWkbcyWNzbWPZmZW\nn5xnNpOAUVVl5wL3R8Rg4P70HElDgDHAwWmdq9NXGwBcA5wBDE6PSp2nAysiYhBwBXBZqqs3MB44\nChgGjC+HmpmZdb1sYRMRDwPLq4pHA5X7rF0PHFcqvzki1kbEC0AbMEzSPsCeEfFo+lrqG6rWqdR1\nKzA8nfWMBKZFxPKIWEHxHTzVoWdmZl2oq6/Z9IuIxWn6JaBfmm4B5peWW5DKWtJ0dflG60TEOmAl\nsHeNuszMrEEaNkAgnak09BY4ksZJapXUumzZskY2xcysqXV12CxJXWOkn0tT+UKgf2m5/VLZwjRd\nXb7ROpK6Az2BV2rU9Q4RcW1EDI2IoX379t2C3TIzs1q6OmymApXRYWOBO0vlY9IIswMoBgJMT11u\nqyQdna7HnFq1TqWuE4AH0tnSvcAISb3SwIARqczMzBqk3m/q3GSSbgKOAfpIWkAxQuxSYIqk04EX\ngZMAImK2pCnAHGAdcFZErE9VnUkxsm0X4J70ALgOmCypjWIgwphU13JJlwAz0nIXR0T1QAUzM+tC\n2cImIk7uYNbwDpafAExop7wVOKSd8jXAiR3UNRGYWHdjzcwsK99BwMzMsnPYmJlZdg4bMzPLzmFj\nZmbZOWzMzCw7h42ZmWXnsDEzs+wcNmZmlp3DxszMsnPYmJlZdg4bMzPLzmFjZmbZOWzMzCw7h42Z\nmWXnsDEzs+wcNmZmlp3DxszMsnPYmJlZdg4bMzPLzmFjZmbZOWzMzCw7h42ZmWXnsDEzs+wcNmZm\nlp3DxszMsnPYmJlZdg4bMzPLriFhI2mepFmSZkpqTWW9JU2TNDf97FVa/jxJbZKelTSyVH5EqqdN\n0lWSlMp7SLollT8maWBX76OZmb2tkWc2n4yIwyJiaHp+LnB/RAwG7k/PkTQEGAMcDIwCrpbULa1z\nDXAGMDg9RqXy04EVETEIuAK4rAv2x8zMOvBu6kYbDVyfpq8HjiuV3xwRayPiBaANGCZpH2DPiHg0\nIgK4oWqdSl23AsMrZz1mZtb1GhU2Adwn6XFJ41JZv4hYnKZfAvql6RZgfmndBamsJU1Xl2+0TkSs\nA1YCe2/tnTAzs/p0b9B2PxYRCyW9B5gm6ZnyzIgISZG7ESnoxgEMGDAg9+bMzLZbDTmziYiF6edS\n4HZgGLAkdY2Rfi5Niy8E+pdW3y+VLUzT1eUbrSOpO9ATeKWddlwbEUMjYmjfvn23zs6Zmdk7dHnY\nSNpN0h6VaWAE8BQwFRibFhsL3JmmpwJj0gizAygGAkxPXW6rJB2drsecWrVOpa4TgAfSdR0zM2uA\nRnSj9QNuT9fruwM/j4hfSZoBTJF0OvAicBJARMyWNAWYA6wDzoqI9amuM4FJwC7APekBcB0wWVIb\nsJxiNJuZmTVIl4dNRDwPHNpO+SvA8A7WmQBMaKe8FTiknfI1wIlb3FgzM9sq3k1Dn83MrEk5bMzM\nLDuHjZmZZeewMTOz7Bw2ZmaWncPGzMyyc9iYmVl2DhszM8vOYWNmZtk5bMzMLDuHjZmZZeewMTOz\n7Bw2ZmaWncPGzMyyc9iYmVl2DhszM8vOYWNmZtk5bMzMLDuHjZmZZeewMTOz7Bw2ZmaWncPGzMyy\nc9iYmVl2DhszM8vOYWNmZtk5bMzMLDuHjZmZZeewMTOz7Jo6bCSNkvSspDZJ5za6PWZm26umDRtJ\n3YAfAJ8BhgAnSxrS2FaZmW2fmjZsgGFAW0Q8HxFvADcDoxvcJjOz7VL3RjcgoxZgfun5AuCo8gKS\nxgHj0tPVkp7torZtD/oALze6EZ3RZY1ugTXINvH7uY3Yv56FmjlsOhUR1wLXNrodzUhSa0QMbXQ7\nzNrj38+u18zdaAuB/qXn+6UyMzPrYs0cNjOAwZIOkLQTMAaY2uA2mZltl5q2Gy0i1kn6GnAv0A2Y\nGBGzG9ys7Ym7J+3dzL+fXUwR0eg2mJlZk2vmbjQzM3uXcNiYmVl2DhszM8uuaQcIWNeS9CcUd2ho\nSUULgakR8XTjWmVm7xY+s7EtJukcitsBCZieHgJu8g1Q7d1M0mmNbsP2wqPRbItJ+n/AwRHxZlX5\nTsDsiBjcmJaZ1Sbp9xExoNHt2B64G822hg3AvsCLVeX7pHlmDSPpyY5mAf26si3bM4eNbQ1nA/dL\nmsvbNz8dAAwCvtawVpkV+gEjgRVV5QL+s+ubs31y2NgWi4hfSTqI4msdygMEZkTE+sa1zAyAfwd2\nj4iZ1TMkPdj1zdk++ZqNmZll59FoZmaWncPGzMyyc9iYNYCk90q6WdJzkh6XdLekgyQ91ei2meXg\nAQJmXUySgNuB6yNiTCo7FA/DtSbmMxuzrvdJ4M2I+GGlICL+m7eHjSNpoKTfSnoiPf40le8j6WFJ\nMyU9JenjkrpJmpSez5L0t12/S2a1+czGrOsdAjzeyTJLgWMjYo2kwcBNwFDgC8C9ETFBUjdgV+Aw\noCUiDgGQtFe+ppttHoeN2bvTjsD3JR0GrAcOSuUzgImSdgTuiIiZkp4HDpT0r8BdwK8b0mKzGtyN\nZtb1ZgNHdLLM3wJLgEMpzmh2AoiIh4E/o/in2UmSTo2IFWm5B4GvAj/J02yzzeewMet6DwA9JI2r\nFEj6ENC/tExPYHFEbABOAbql5fYHlkTEjylC5XBJfYAdIuI24ALg8K7ZDbP6uRvNrItFREg6Hrgy\nfT3DGmAexT3mKq4GbpN0KvAr4LVUfgzw95LeBFYDp1LcIuinkiofHs/LvhNmm8i3qzEzs+zcjWZm\nZtk5bMzMLDuHjZmZZeewMTOz7Bw2ZmaWncPGzMyyc9iYmVl2DhszM8vu/wPnjAchopi5BAAAAABJ\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111c6b590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Print the number of positive and negative samples\n",
    "count = 0\n",
    "for entry in labels:\n",
    "    if entry == 1:\n",
    "        count += 1\n",
    "print(\"The dataset has {} samples, {} of which are in the positive class\".format(len(labels), count))\n",
    "\n",
    "count_classes = pd.value_counts(raw_data['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Credit Card Class Distribution Histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram clearly demonstrates that with this particular dataset, pertaining to credit card fraud, there is an inbalance between the classes. Concretely we have significantly more negative samples, or not fraud, than we do examples of fraud. This is not entirely unexpected as the data contains all credit card transactions for a 24h period - however the disparity between the class distribution could skew results. For instance, a model that simply predicted nothing as fraud would be 99% accurate!!!\n",
    "\n",
    "The positive 'fraud' class makes up ~0.17% of the raw data. Before attempting to model the data in any way - it's important to have a more balanced ratio between the classes. There are several ways of doing this: \n",
    "\n",
    "* Collect more data for the positive class\n",
    "* Undersample the negative class\n",
    "* Changing the performance metric \n",
    "\n",
    "As I've discussed previously, Financial Services data is difficult to come by so the notion of growing my dataset is not feasible within the scope of this project. In order to reduce the risk of potential overfitting I will reduce the number of samples in the negative class. I will collect a random selection of negative, not fraud, samples and attempt to balance the ratio of positive/negative samples.\n",
    "\n",
    "However this process will induce an error in the dataset. The 'Time' feature tells me the number of elapsed seconds between transactions, when I randomly remove some of the samples this value will no longer be correct. In order to mitigate this I will simply exclude the 'Time' feature from the dataset used for testing. \n",
    "\n",
    "I have also observed that the 'Amount' feature is not normalised - I will scale this feature in the new dataset to mitigate any issue arising from the scale of the Amount feature. \n",
    "\n",
    "In the code block below I reduce the number of samples in the negative class to ~500 and remove the 'Time' feature which would have been corrupted by this reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 992 samples. Of which 492 are fraud and 500 are not\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x113b90b10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAETCAYAAAAs4pGmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6fj+UNPBjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111c6b510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separate positive and negative classes\n",
    "negative_class = raw_data.loc[raw_data['Class'] == 0]\n",
    "positive_class = raw_data.loc[raw_data['Class'] == 1]\n",
    "\n",
    "# Randomly sample the negative class\n",
    "random_selection = negative_class.sample(500)\n",
    "\n",
    "frames = [random_selection, positive_class]\n",
    "# Concatenate the positive class and the undersampled negative class\n",
    "small_data = pd.concat(frames)\n",
    "# Normalise the amount feature\n",
    "small_data['Normalised_Amount'] = StandardScaler().fit_transform(small_data['Amount'].values.reshape(-1, 1))\n",
    "small_data = small_data.drop(['Time','Amount'],axis=1)\n",
    "\n",
    "# Drop the Time feature and extract the class label\n",
    "small_data_features = small_data.drop('Class', axis=1)\n",
    "small_data_labels = small_data['Class']\n",
    "\n",
    "# Print the length of the new dataset/classes\n",
    "print(\"The dataset contains {} samples. Of which {} are fraud and {} are not\".format(len(small_data), len(positive_class), len(random_selection)))\n",
    "\n",
    "# Draw histogram of new distribution\n",
    "count_classes = pd.value_counts(small_data['Class'], sort = True).sort_index()\n",
    "count_classes.plot(kind = 'bar')\n",
    "plt.title(\"Credit Card Class Distribution Histogram\")\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the undersampling of the raw data I have a much more even distribution of both classes, which is clearly illustrated by the histogram. The dataset now has 992 samples, as opposed to the original 30k from the raw data. The distribution is not exactly 50:50 between both classes, there are 492 instances of fraud (the entire positive class from the original dataset) and 500 randomly selected rows from the negative class.\n",
    "\n",
    "It was important to ensure that the rows were selected at random to eliminate any potential experimenter bias. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling The Data\n",
    "\n",
    "\n",
    "## Logistic Regression Classifier\n",
    "\n",
    "Now I have read the data into a DataFrame, defined a function for computing the accuracy of the model and balanced the class distribution it is now time to start modeling the data. As discussed previously, the first technique I will use is SKLearn's LogisticRegression classifier. This model will serve as a benchmark for the more complicated models I explore later in this study. \n",
    "\n",
    "The code cell below defines the LogisticRegression classifier and reports it's accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.97, Recall: 0.92, Fscore: 0.94\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Split the data into training and testing sets using the given feature as the target\n",
    "X_train, X_test, y_train, y_test = train_test_split(small_data_features, small_data_labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Create a logistic regression classifier and fit it to the training set\n",
    "bmark_clf = LogisticRegression(random_state=5)\n",
    "bmark_clf.fit(X_train, y_train)\n",
    "\n",
    "# Report the score of the prediction using the testing set\n",
    "y_pred = bmark_clf.predict(X_test)\n",
    "score = accuracy(y_test, y_pred)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The benchmark model has an FScore (accuracy) of 0.94. This will act as a reference point/standard to compare all other techniques against. I think the fact that the LogisticRegression classifier has performed so well in this instance is testiment to the PCA conducted on the dataset. \n",
    "\n",
    "## SVM - Support Vector Machines\n",
    "\n",
    "The next model I will test is the SVM classifier in SKLearn. An SVM model represents the examples as points in space. These points are then mapped so that examples from both classes (positive/negative, fraud/not fraud) are divided by a clear gap and the margin is maximised. New samples are then mapped into the same plane and a classification is made based on which side of the boundry they fall. SVMs are 'non-probablistic binary linear classifiers' and thus I would expect them to perform well for this use case. \n",
    "\n",
    "The code cell below trains an SVM for the Credit Card fraud data and then analyses the model's performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.96, Recall: 0.91, Fscore: 0.93\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create a logistic regression classifier and fit it to the training set\n",
    "svc_clf = SVC()\n",
    "svc_clf.fit(X_train, y_train)\n",
    "\n",
    "# Report the score of the prediction using the testing set\n",
    "y_pred = svc_clf.predict(X_test)\n",
    "score = accuracy(y_test, y_pred)\n",
    "\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Benchmark\n",
    "\n",
    "The SVM model actually performed less well than the LogisticRegression classifier for this dataset. The SVM model was not able to detect as many instances of fraud as the benchmark. In the benchmark test, 94% of fraud cases were classified correctly. The SVM recalled ~2% less than this - making it less effective. \n",
    "\n",
    "### Comments\n",
    "\n",
    "In practice SVMs and Logistic Regression's perform comparably. The performance of the SVM shows that the data is linearly sepearable, and there are no significant outliers skewing the model. \n",
    "\n",
    "The SVM algorithm implemented in SKLearn is based on libsvm. This technique is significantly more 'geometrically motivated' than the benchmark model. The goal here is not to define a probabilistic model, but to find an optimal separating hyperplane. The fact that we are seeing similar results between this and the benchmark simply indicates that the Logistic Regression is performing well and producing a 'wide margin' classifier, which is the goal of the SVM.\n",
    "\n",
    "## Neural Network\n",
    "\n",
    "I will use the TensorFlow framework to define a Neural Network architecture for detecting credit card fraud. This will allow me to explore the performance of NN architecture when compared directly to more basic Machine Learning algorithms. \n",
    "\n",
    "The code cell below defines the network itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "sess = tf.Session()\n",
    "\n",
    "# Convert DataFrames to Matricies for TF\n",
    "trainX = X_train.as_matrix()\n",
    "trainY = y_train.as_matrix()\n",
    "testX = X_test.as_matrix()\n",
    "testY = y_test.as_matrix()\n",
    "\n",
    "# Reshape Labels\n",
    "trainYShape = trainY.shape\n",
    "trainY = trainY.reshape(trainYShape[0], 1)\n",
    "trainY = np.concatenate((1-trainY, trainY), axis=1)\n",
    "testYShape = testY.shape\n",
    "testY = testY.reshape(testYShape[0], 1)\n",
    "testY = np.concatenate((1-testY, testY), axis=1)\n",
    "\n",
    "# Multiplier maintains fixed ratio of nodes between each layer\n",
    "mulitplier = 1.5 \n",
    "\n",
    "# Number of nodes in hidden layer 1\n",
    "hidden_nodes1 = 15\n",
    "hidden_nodes2 = round(hidden_nodes1 * mulitplier)\n",
    "hidden_nodes3 = round(hidden_nodes2 * mulitplier)\n",
    "\n",
    "# Input (features from dataset)\n",
    "X_ = tf.placeholder(tf.float32, [None, 29]) \n",
    "\n",
    "# Layer 1\n",
    "W1 = tf.Variable(tf.zeros([29, hidden_nodes1]))\n",
    "b1 = tf.Variable(tf.zeros([hidden_nodes1]))\n",
    "y1 = tf.nn.sigmoid(tf.matmul(X_, W1) + b1)\n",
    "\n",
    "# Layer 2\n",
    "W2 = tf.Variable(tf.zeros([hidden_nodes1, hidden_nodes2]))\n",
    "b2 = tf.Variable(tf.zeros([hidden_nodes2]))\n",
    "y2 = tf.nn.sigmoid(tf.matmul(y1, W2) + b2)\n",
    "\n",
    "# Layer 3\n",
    "W3 = tf.Variable(tf.zeros([hidden_nodes2, hidden_nodes3])) \n",
    "b3 = tf.Variable(tf.zeros([hidden_nodes3]))\n",
    "y3 = tf.nn.sigmoid(tf.matmul(y2, W3) + b3)\n",
    "\n",
    "# Layer 4\n",
    "W4 = tf.Variable(tf.zeros([hidden_nodes3, 2])) \n",
    "b4 = tf.Variable(tf.zeros([2]))\n",
    "y4 = tf.nn.softmax(tf.matmul(y3, W4) + b4)\n",
    "\n",
    "# Output (classification) layer\n",
    "y = y4\n",
    "y_ = tf.placeholder(tf.float32, [None,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "The following hyperparameters can be adjusted to tune network performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 30000 \n",
    "display_step = 5000\n",
    "batch_size = trainY.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "\n",
    "I have elected to use TensorFlow's AdamOptimiser for the cost function. This is because the AdamOptimizer will automatically control the learning rate, in effect having a larger step size at each epoch and cause convergence more quickly. As the Adam algorithm determines this step size automatically, this convergence can be achieved without fine tuning which makes the Adam Optimizer ideal for this scenario. \n",
    "\n",
    "The downside to this is that the Adam algorithm will require more computation for each paramter at every epoch so that the moving averages and scaled gradient can be calculated. This will cause an increase in the training time! The model itself will also be ~3x larger than one created using the tf.train.GradientDescentOptimizer. \n",
    "\n",
    "I am also using a mean squared error loss function. While classically Cross-Entropy would be prefered over MSE for classification challenges such as this, I feel that due to the performance of the linear classifiers in earlier tests MSE will perform equally well in this case as it is ideally suited to regression problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cross Entropy cost function\n",
    "cost_fn = tf.reduce_sum(tf.pow(y_ - y, 2))/(2*batch_size)\n",
    "\n",
    "# Define model optimiser\n",
    "optimiser = tf.train.AdamOptimizer(learning_rate).minimize(cost_fn)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Initialize variables and tensorflow session\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now that I have defined the Neural Network in TensorFlow, established a cost function, optimiser and initialised the TensorFlow variables/placeholders. The next step is to train the Model itself. In the following code block I feed the dataset to the TensorFlow model and report the Accuracy/Loss for the defined Epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 0  Test Accuracy =  0.495968 Test Loss = 0.04171\n",
      "Training Step: 5000  Test Accuracy =  0.935484 Test Loss = 0.00950934\n",
      "Training Step: 10000  Test Accuracy =  0.935484 Test Loss = 0.0101857\n",
      "Training Step: 15000  Test Accuracy =  0.935484 Test Loss = 0.0105022\n",
      "Training Step: 20000  Test Accuracy =  0.931452 Test Loss = 0.0111504\n",
      "Training Step: 25000  Test Accuracy =  0.931452 Test Loss = 0.011273\n",
      "Training Step: 30000  Test Accuracy =  0.931452 Test Loss = 0.011311\n"
     ]
    }
   ],
   "source": [
    "# Record Accuracy/Cost values for visualisation\n",
    "nn_training_accuracy = [] \n",
    "nn_training_cost = [] \n",
    "nn_test_accuracy = [] \n",
    "nn_test_cost = [] \n",
    "\n",
    "# For every training iteration\n",
    "for iteration in range(training_epochs+1):\n",
    "    # Take the classifier, and feed it the input data\n",
    "    sess.run([optimiser, accuracy, cost_fn], feed_dict={X_: trainX, y_: trainY})\n",
    "    # Every 5000 runs, determine accuracy by feeding model the test data\n",
    "    if iteration % display_step == 0:\n",
    "        epoch_train_accuracy, epoch_train_loss = sess.run([accuracy, cost_fn], feed_dict={X_: trainX, y_: trainY})\n",
    "        epoch_test_accuracy, epoch_test_loss = sess.run([accuracy, cost_fn], feed_dict={X_: testX, y_: testY})\n",
    "        print(\"Training Step: \"+ str(iteration) + \"  Test Accuracy =  \" + str(epoch_test_accuracy) + \" Test Loss = \" + str(epoch_test_loss))\n",
    "        nn_test_accuracy.append(epoch_test_accuracy)\n",
    "        nn_test_cost.append(epoch_test_loss)\n",
    "        # Stored for visualisation\n",
    "        nn_training_accuracy.append(epoch_train_accuracy)\n",
    "        nn_training_cost.append(epoch_train_loss)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaulating The Model\n",
    "\n",
    "Without any hyperparameter optimisation we can see that the Neural Network Model achieves a 90% accuracy for the input dataset. In order to gain a better intuition of what is happening during the training phase, the following code cell renders a visualisation of the loss/accuracy sampled during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJcCAYAAAA7Pup5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b65cad0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Render visualisation of Training loss and accuracy\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,10))\n",
    "\n",
    "ax1.plot(nn_training_accuracy)\n",
    "ax1.plot(nn_training_cost)\n",
    "ax1.set_title('Training Accuracy/Loss')\n",
    "\n",
    "ax2.plot(nn_test_accuracy)\n",
    "ax2.plot(nn_test_cost)\n",
    "ax2.set_title('Test Accuracy/Loss')\n",
    "\n",
    "plt.xlabel('Epoch(s) (x100)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Comparison to Benchmark\n",
    "\n",
    "Interestingly the benchmark model and Neural Network actually performed equivalently for this dataset. Both models were able to predict the instances of fraud in the test set with ~94% accuracy. I think this result illustrates two points:\n",
    "\n",
    "1. That Neural Networks perform well in comparison to more basic models for a variety of workloads\n",
    "2. The Credit Card Fraud dataset used in this experiment is clearly linearly separable. Thus LogisticRegression/other linear classifiers will perform extrodinarly well. This would not be the case if the dataset was less linearly seperable. \n",
    "\n",
    "I think it is also important to note that this is a very simple Neural Network implementation, it would be possible to further optimise this network with hyperparamter tuning/more hidden layers. \n",
    "\n",
    "### Comments\n",
    "\n",
    "The visualisations above show that for both our training and test datasets the Neural Network improved considerably over the training iterations. Being able to visualise both the training/test loss in this way is a powerful tool as it allows us to concretely identify instances of overfitting. As the system is performing equally well for both datasets we can infer that the Neural Network model has not overfit the data and has the ability to generalise well. \n",
    "\n",
    "It is also clear from these visualisations that the Neural Network plateaued at ~93% accuracy for both training and test datasets. In order to further improve the accuracy we would need to change the network definition or modify the hyperparameters/loss function. \n",
    "\n",
    "\n",
    "## AutoEncoders with TensorFlow\n",
    "\n",
    "\n",
    "To conclude this report I will explore how AutoEncoders can be used to define Neural Networks capabile of identifying credit card fraud. My original intention was to apply Transfer Learning of an existing Neural Network definition to this problem and evaluate it's performance - however after conducting this work, and gaining a better understanding of the dataset I feel that using AutoEncoders is a better academic discussion. \n",
    "\n",
    "What makes AutoEncoders so interesting is that they are an instance of ** Unsupervised Learning **. Unlike all of the other methods discussed previously, this technique does not require a subset of labeled data. When you consider the wider domain problem, that is to detect fraud at enterprise scale, not having the prerequisite of labelling your existing dataset to identify the model is a really powerful tool. \n",
    "\n",
    "Because we are now working with unlabled data, I will revert to use the unbalanced dataset from the raw data but without the label that classifies the dataset. I will then work with the assertion that no such labels exist for the data. \n",
    "\n",
    "So what are AutoEncoders? \n",
    "\n",
    "An autoencoder, which can sometimes be refered to as an autoassociator or Diabolo network, is an Artificial Neural Network which can be used for unsupervised learning. This technique aims to learn an encoding for a set of data. The simplest architecture for an autoencoder is a feedforward non-recurrent neural network. This network has an input layer, output layer and N hidden layers connecting these two. The output layer has the same number of nodes as the input layer with the purpose of redeveloping its own inputs as opposed to predicting the target value. \n",
    "\n",
    "\n",
    "Some typical use cases of AutoEncoders include: \n",
    "\n",
    "* Data Denoising - reducing the amount of noise/error in an image for example. \n",
    "* Dimensionality Reduction - for instance in this example, where we look to perform feature extraction and reduce the number of input features to make understanding the data a less complex task. \n",
    "* VAE (Variational AutoEncoders) - Here we learn the parameters of the probability distribution by modeling the input data as opposed to learning some function (as is the case with traditional AutoEncoders) \n",
    "\n",
    "\n",
    "How does it work? \n",
    "\n",
    "I'll use an AutoEncoder to 'learn' the common/frequent patterns that are shared by the significant proportion of the training data. The Root Mean Squared Error will be large for the data that does not conform to these patterns. It's this step that is ideal for our use case, we have a large amount of 'not fraud' data which we will use to build a pattern of the 'normal' and anything that lies outside that will have a large RMSE which we can use to infer that these points are 'fraud' or examples of the positive class. \n",
    "\n",
    "There are two approaches: \n",
    "\n",
    "1. We select a threshold for the error and assume everything over a certain value is fraud\n",
    "2. We rank all the RMSE values and take the largest (say 0.17%) and state that these are fraudulent\n",
    "\n",
    "I appreciate I'm cheating a little bit with point two because I know that 0.17% of my dataset is fraudulent. However it would be possible to narrow down on this value in the real world via repitition. \n",
    "\n",
    "\n",
    "### Building The Dataset\n",
    "\n",
    "As discussed, we need to work with the whole dataset for this technique minus the label. In the code cell below I normalise the Amount value and remove the label so we're working with an instance of unsupervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 284807 samples.\n"
     ]
    }
   ],
   "source": [
    "# Compose a dataset of unlabled data by removing the Class feature. \n",
    "ae_data = features\n",
    "\n",
    "# Normalise the amount feature\n",
    "ae_data['Normalised_Amount'] = StandardScaler().fit_transform(ae_data['Amount'].values.reshape(-1, 1))\n",
    "ae_data = ae_data.drop(['Time','Amount'],axis=1)\n",
    "\n",
    "# Train/Test split this data..\n",
    "X_train_ae, X_test_ae, y_train_ae, y_test_ae = train_test_split(ae_data, labels, test_size=0.25, random_state=42)\n",
    "\n",
    "# Print the length of the new dataset/classes\n",
    "print(\"The dataset contains {} samples.\".format(len(ae_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ae_data dataset includes all the rows from the raw data without the \"Class\" feature. As before I have also normalised the Amount column for consistency. I have performed a train/test split on the data (reserving 25% for testing) in order to perform Cross Validation.  \n",
    "\n",
    "### Building the Model\n",
    "\n",
    "The next step is to build the AutoEncoder itself. In the code cell below you can see that I have defined the hyperparameters which, with the exception of the learning rate, are the same as the pervious TensorFlow experiment. I then define the network parameters, specifying the number of features in the hidden layers and the feature dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "learning_rate = 0.01\n",
    "training_epochs = 30000\n",
    "display_step = 500\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 15 \n",
    "n_hidden_2 = 5 \n",
    "n_input = X_train_ae.shape[1]  \n",
    "X_ = tf.placeholder(\"float\", [None, n_input])\n",
    " \n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([n_hidden_2, n_hidden_1])),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([n_hidden_1, n_input])),\n",
    "}\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([n_input])),\n",
    "}\n",
    " \n",
    "# Encoder\n",
    "def encoder(x):\n",
    "    # Encoder Hidden layer with tanh activation #1\n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['encoder_h1']),\n",
    "              biases['encoder_b1']))\n",
    "    # Decoder Hidden layer with tanh activation #2\n",
    "    layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['encoder_h2']),\n",
    "              biases['encoder_b2']))\n",
    "    return layer_2\n",
    " \n",
    "# Decoder\n",
    "def decoder(x):\n",
    "    # Encoder Hidden layer with tanh activation #1\n",
    "    layer_1 = tf.nn.tanh(tf.add(tf.matmul(x, weights['decoder_h1']),\n",
    "            biases['decoder_b1']))\n",
    "    # Decoder Hidden layer with tanh activation #2\n",
    "    layer_2 = tf.nn.tanh(tf.add(tf.matmul(layer_1, weights['decoder_h2']),\n",
    "            biases['decoder_b2']))\n",
    "    return layer_2\n",
    " \n",
    "# Construct model\n",
    "encoder_op = encoder(X_)\n",
    "decoder_op = decoder(encoder_op)\n",
    " \n",
    "# Prediction\n",
    "y_pred_ae = decoder_op\n",
    "y_true_ae = X_\n",
    " \n",
    "# Measure model accuracy\n",
    "correct_prediction = tf.equal(tf.argmax(y_true_ae,1), tf.argmax(y_pred_ae,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "# Define loss and optimizer, minimize the squared error\n",
    "cost = tf.reduce_mean(tf.pow(y_true_ae - y_pred_ae, 2))\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable X is the placeholder for the input data. The two Dictionaries (Weights, Biases) hold all of the parameters that will be optimised during the training phase. Each layer of the AutoEncoder uses a tanh activation function, this outputs values as: \n",
    "\n",
    "value < 0 < value\n",
    "\n",
    "Thus is ideally suited for this use case. \n",
    "\n",
    "Unlike the Neural Network experiement above I will use a Root Mean Squared Error (RMSE) cost function to optimise the parameters of the model. \n",
    "\n",
    "### Training the Model\n",
    "\n",
    "The code cell below trains the AutoEncoder. Every `display_step` iterations we assess the accuracy of the model against the training and test data. After the training is completed we will be able to plot these values on a chart to assess: \n",
    "\n",
    "1. The performance of the AutoEncoder/Model during the training phase\n",
    "2. How the AutoEncoder performs for this workload compared to the previous experiments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 0  Test Accuracy =  0.0786073 Test Loss = 1.82776\n",
      "Training Step: 500  Test Accuracy =  0.178871 Test Loss = 0.831145\n",
      "Training Step: 1000  Test Accuracy =  0.253097 Test Loss = 0.774786\n",
      "Training Step: 1500  Test Accuracy =  0.242718 Test Loss = 0.753736\n",
      "Training Step: 2000  Test Accuracy =  0.414104 Test Loss = 0.725808\n",
      "Training Step: 2500  Test Accuracy =  0.424553 Test Loss = 0.712971\n",
      "Training Step: 3000  Test Accuracy =  0.428612 Test Loss = 0.705381\n",
      "Training Step: 3500  Test Accuracy =  0.443667 Test Loss = 0.694897\n",
      "Training Step: 4000  Test Accuracy =  0.442923 Test Loss = 0.685507\n",
      "Training Step: 4500  Test Accuracy =  0.447516 Test Loss = 0.681742\n",
      "Training Step: 5000  Test Accuracy =  0.449412 Test Loss = 0.676874\n",
      "Training Step: 5500  Test Accuracy =  0.449327 Test Loss = 0.677602\n",
      "Training Step: 6000  Test Accuracy =  0.455619 Test Loss = 0.679623\n",
      "Training Step: 6500  Test Accuracy =  0.456139 Test Loss = 0.674504\n",
      "Training Step: 7000  Test Accuracy =  0.468681 Test Loss = 0.666919\n",
      "Training Step: 7500  Test Accuracy =  0.463737 Test Loss = 0.663037\n",
      "Training Step: 8000  Test Accuracy =  0.463723 Test Loss = 0.663547\n",
      "Training Step: 8500  Test Accuracy =  0.474453 Test Loss = 0.663879\n",
      "Training Step: 9000  Test Accuracy =  0.468793 Test Loss = 0.656916\n",
      "Training Step: 9500  Test Accuracy =  0.473372 Test Loss = 0.661751\n",
      "Training Step: 10000  Test Accuracy =  0.473821 Test Loss = 0.655106\n",
      "Training Step: 10500  Test Accuracy =  0.475211 Test Loss = 0.655941\n",
      "Training Step: 11000  Test Accuracy =  0.479312 Test Loss = 0.655453\n",
      "Training Step: 11500  Test Accuracy =  0.479439 Test Loss = 0.652445\n",
      "Training Step: 12000  Test Accuracy =  0.476826 Test Loss = 0.657237\n",
      "Training Step: 12500  Test Accuracy =  0.48434 Test Loss = 0.651308\n",
      "Training Step: 13000  Test Accuracy =  0.4817 Test Loss = 0.651361\n",
      "Training Step: 13500  Test Accuracy =  0.487037 Test Loss = 0.647769\n",
      "Training Step: 14000  Test Accuracy =  0.491742 Test Loss = 0.647477\n",
      "Training Step: 14500  Test Accuracy =  0.495534 Test Loss = 0.646969\n",
      "Training Step: 15000  Test Accuracy =  0.496334 Test Loss = 0.648384\n",
      "Training Step: 15500  Test Accuracy =  0.485239 Test Loss = 0.646289\n",
      "Training Step: 16000  Test Accuracy =  0.492599 Test Loss = 0.642303\n",
      "Training Step: 16500  Test Accuracy =  0.478287 Test Loss = 0.64737\n",
      "Training Step: 17000  Test Accuracy =  0.47854 Test Loss = 0.643585\n",
      "Training Step: 17500  Test Accuracy =  0.47906 Test Loss = 0.648998\n",
      "Training Step: 18000  Test Accuracy =  0.485941 Test Loss = 0.641044\n",
      "Training Step: 18500  Test Accuracy =  0.477487 Test Loss = 0.639742\n",
      "Training Step: 19000  Test Accuracy =  0.484453 Test Loss = 0.637103\n",
      "Training Step: 19500  Test Accuracy =  0.485941 Test Loss = 0.636727\n",
      "Training Step: 20000  Test Accuracy =  0.48486 Test Loss = 0.636391\n",
      "Training Step: 20500  Test Accuracy =  0.482824 Test Loss = 0.64168\n",
      "Training Step: 21000  Test Accuracy =  0.485436 Test Loss = 0.635504\n",
      "Training Step: 21500  Test Accuracy =  0.485057 Test Loss = 0.635257\n",
      "Training Step: 22000  Test Accuracy =  0.487318 Test Loss = 0.636469\n",
      "Training Step: 22500  Test Accuracy =  0.489017 Test Loss = 0.634276\n",
      "Training Step: 23000  Test Accuracy =  0.487697 Test Loss = 0.634412\n",
      "Training Step: 23500  Test Accuracy =  0.485464 Test Loss = 0.635892\n",
      "Training Step: 24000  Test Accuracy =  0.487163 Test Loss = 0.632602\n",
      "Training Step: 24500  Test Accuracy =  0.481082 Test Loss = 0.63398\n",
      "Training Step: 25000  Test Accuracy =  0.484298 Test Loss = 0.635566\n",
      "Training Step: 25500  Test Accuracy =  0.486826 Test Loss = 0.634327\n",
      "Training Step: 26000  Test Accuracy =  0.482795 Test Loss = 0.633313\n",
      "Training Step: 26500  Test Accuracy =  0.491026 Test Loss = 0.631636\n",
      "Training Step: 27000  Test Accuracy =  0.483371 Test Loss = 0.633851\n",
      "Training Step: 27500  Test Accuracy =  0.483624 Test Loss = 0.631797\n",
      "Training Step: 28000  Test Accuracy =  0.48941 Test Loss = 0.634301\n",
      "Training Step: 28500  Test Accuracy =  0.491124 Test Loss = 0.634289\n",
      "Training Step: 29000  Test Accuracy =  0.485899 Test Loss = 0.631958\n",
      "Training Step: 29500  Test Accuracy =  0.486236 Test Loss = 0.632856\n",
      "Training Step: 30000  Test Accuracy =  0.492613 Test Loss = 0.630019\n",
      "Optimisation Finished!!\n"
     ]
    }
   ],
   "source": [
    "# Initialise the variables\n",
    "init = tf.global_variables_initializer()    \n",
    "\n",
    "# Store train/test accuracy for visualisation\n",
    "ae_training_accuracy = []\n",
    "ae_training_cost = []\n",
    "ae_test_accuracy = []\n",
    "ae_test_cost = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    # Training loop\n",
    "    for iteration in range(training_epochs+1):\n",
    "        # Take the classifier, and feed it the input data\n",
    "        sess.run([optimizer, cost], feed_dict={X_: X_train_ae})\n",
    "        # Every 50 runs, determine accuracy by feeding model the test data\n",
    "        if iteration % display_step == 0:\n",
    "            epoch_train_accuracy, epoch_train_loss = sess.run([accuracy, cost], feed_dict={X_: X_train_ae})\n",
    "            epoch_test_accuracy, epoch_test_loss = sess.run([accuracy, cost], feed_dict={X_: X_test_ae})\n",
    "            print(\"Training Step: \"+ str(iteration) + \"  Test Accuracy =  \" + str(epoch_test_accuracy) + \" Test Loss = \" + str(epoch_test_loss))\n",
    "            # Stored for visualisation\n",
    "            ae_training_accuracy.append(epoch_train_accuracy)\n",
    "            ae_training_cost.append(epoch_train_loss)\n",
    "            ae_test_accuracy.append(epoch_test_accuracy)\n",
    "            ae_test_cost.append(epoch_test_loss)\n",
    "    print(\"Optimisation Finished!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaulating The Model\n",
    "\n",
    "Using an equivalent number of Epochs as the Neural Network we can see that the AutoEncoder achieved a 49.3% accuracy for the training dataset. The following code cell will render a visualisation of the accuracy and loss for the train/test datasets achieved during the training process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAJcCAYAAADD3hGOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0gAKXSIiIiINoNAlIiIi0gD/H7CCN9mJn++LAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b75e750>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AutoEncoder achieved a maximum accuracy of 49.2612570524% on the validation dataset.\n"
     ]
    }
   ],
   "source": [
    "# Render visualisation of Training loss and accuracy\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True, figsize=(10,10))\n",
    "\n",
    "ax1.plot(ae_training_accuracy)\n",
    "ax1.plot(ae_training_cost)\n",
    "ax1.set_title('AE Training Accuracy/Loss')\n",
    "\n",
    "ax2.plot(ae_test_accuracy)\n",
    "ax2.plot(ae_test_cost)\n",
    "ax2.set_title('AE Test Accuracy/Loss')\n",
    "\n",
    "plt.xlabel('Epoch(s) (x100)')\n",
    "plt.show()\n",
    "\n",
    "print(\"The AutoEncoder achieved a maximum accuracy of {}% on the validation dataset.\".format(ae_test_accuracy[len(ae_test_accuracy) -1] * 100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison to Benchmark\n",
    "\n",
    "On the Credit Card Fraud dataset AutoEncoders did not perform as well as Neural Networks or the benchmark model. The AutoEncoder achieved an ultimate accuracy of 49.3% on the Test dataset. Unfortunately, many of the steps I would subsequently have taken to optimise this model cannot be done due to the feature engineering conducted on the dataset. With this in mind it is not possible to make an accurate inference about the applicability of AutoEncoders in this domain. In order to more rigourously test this model I could follow the following proceedure: \n",
    "\n",
    "1. Collect more data (having more samples in the positive class would be beneficial)\n",
    "2. Conduct feature engineering on the raw data. In order to improve the performance of the model I would like to conduct PCA/Feature Engineering on the raw data. However, as previously discussed, due to the nature of the domain this is not possible.\n",
    "3. Hyperparameter optimisation/more hidden layers. \n",
    "\n",
    "\n",
    "### Comments\n",
    "\n",
    "The visualisations above show that for both our training and test datasets the AutoEncoder improved  over the training iterations. Being able to visualise both the training/test loss in this way is a powerful tool as it allows us to concretely identify instances of overfitting. As the system is performing poorly both training and validation datasets we cannot infer whether or not AutoEncoders, and unsupervised learning, could be used to tackle this domain problem. \n",
    "\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "## Reflection\n",
    "\n",
    "With this project I set out with the goal of determining whether or not it was possible to build a system, using Machine Learning that could recognise and detect Credit Card fraud. While, as previously discussed, there were challenges with the dataset caused by privacy concerns I feel that I have successfully demonstrated that it is indeed possible to build such a system. \n",
    "\n",
    "I have used a variety of techniques, from simple `LogisticRegression` classifiers through to more complicated Neural Networks and AutoEncoders to build systems capable of detecting Credit Card Fraud. I have ranked the models in terms of accuracy here: \n",
    "\n",
    "1. `LogisitcRegression` Classifier\n",
    "2. `Neural Network`\n",
    "3. `SVM`\n",
    "4. `AutoEncoder`\n",
    "\n",
    "The LogisticRegression classifier performed really well at this task, achieving an accuracy of 94%. This classifier was capable of detecting fraud in the validation dataset at a very high level of accuracy. This model shows that there is validity in persuing Machine Learning as a technique for identifying Credit Card fraud for the entire industry. \n",
    "\n",
    "The fact that LogisticRegression classifier, and SVM performed so well also tells us interesting things about the underlying dataset. These techniques are extremely effective with linearly separable data. That is to say, it is possible to deliniate both classes with a single boundry. \n",
    "\n",
    "As a result of the composition of the dataset simple classifiers performed really well. I was also interested to see how well Neural Networks would manage a binary classification task. The NN Model achieved a relatively similar accuracy to the other classifiers. While it was more complicated to implement the Neural Network, it achieved a similar accuracy and would scale to more complicated problem domains should the data be available. \n",
    "\n",
    "Unfortunatley due to the dimensionality reduction already conducted on the dataset the AutoEncoder did not perform particularly well for this project. In order to improve the system I would need access to more data, and conduct PCA/analysis on the various features. I firmly believe this approach would be applicable to allow Financial Services organisations to leverage unsupervised learning to automatically detect Credit Card Fraud from unlabled data. This would be extremely valuable because in the wild, the data itself is very much unlabled.  \n",
    "\n",
    "\n",
    "## Improvement\n",
    "\n",
    "In this project the dataset has already undergone Dimensionality Reduction, Feature Normalisation and PCA. In reality, Financial Services institutions do not hold their data in this processed format. In order to more accuratley simulate the real world scenario I would require access to this raw data. Unfortunately (or forunately) due to privacy concerns and legislation, it is not possible to observe this information in its raw form. \n",
    "\n",
    "I would also have liked access to more datapoints in the Positive class. Only 0.17% of the data is an instance of Fraud and this ultimately impacts the scalability and ecological validity of the model. As I had to significanlty reduce the dataset size for this analysis there is a high chance of overfitting. If I were to repeat this study I would certainly seek a larger dataset.  \n",
    "\n",
    "Finally I have thoroughly enjoyed this project, and the entire nanodegree program. It has given me a deep insight into Machine Learning, Deep Learning and the challenges faced by Data Scientists. :) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
